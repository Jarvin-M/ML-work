{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 32)        1568      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 32, 32, 96)        49248     \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 32, 32, 96)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 96)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 96)        384       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 16, 16, 128)       196736    \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 16, 16, 256)       524544    \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 16, 16, 128)       524416    \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              33558528  \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "activation_17 (Activation)   (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 1000)              4000      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                16016     \n",
      "_________________________________________________________________\n",
      "activation_18 (Activation)   (None, 16)                0         \n",
      "=================================================================\n",
      "Total params: 55,788,696\n",
      "Trainable params: 55,769,032\n",
      "Non-trainable params: 19,664\n",
      "_________________________________________________________________\n",
      "Train on 840 samples, validate on 210 samples\n",
      "Epoch 1/10\n",
      "840/840 [==============================] - 97s 115ms/step - loss: 1.5966 - acc: 0.5048 - val_loss: 6.5066 - val_acc: 0.3905\n",
      "Epoch 2/10\n",
      "840/840 [==============================] - 96s 114ms/step - loss: 0.8963 - acc: 0.7036 - val_loss: 6.1387 - val_acc: 0.4286\n",
      "Epoch 3/10\n",
      "840/840 [==============================] - 93s 111ms/step - loss: 0.5413 - acc: 0.8036 - val_loss: 0.7441 - val_acc: 0.7286\n",
      "Epoch 4/10\n",
      "840/840 [==============================] - 95s 114ms/step - loss: 0.3821 - acc: 0.8655 - val_loss: 1.8926 - val_acc: 0.6429\n",
      "Epoch 5/10\n",
      "840/840 [==============================] - 93s 111ms/step - loss: 0.4338 - acc: 0.8583 - val_loss: 0.8041 - val_acc: 0.7381\n",
      "Epoch 6/10\n",
      "840/840 [==============================] - 83s 99ms/step - loss: 0.6383 - acc: 0.8036 - val_loss: 12.2116 - val_acc: 0.1381\n",
      "Epoch 7/10\n",
      "840/840 [==============================] - 88s 105ms/step - loss: 0.5169 - acc: 0.8226 - val_loss: 7.8882 - val_acc: 0.1810\n",
      "Epoch 8/10\n",
      "840/840 [==============================] - 91s 108ms/step - loss: 0.4536 - acc: 0.8500 - val_loss: 4.8310 - val_acc: 0.4667\n",
      "Epoch 9/10\n",
      "840/840 [==============================] - 80s 95ms/step - loss: 0.3063 - acc: 0.8869 - val_loss: 2.0006 - val_acc: 0.6952\n",
      "Epoch 10/10\n",
      "840/840 [==============================] - 81s 97ms/step - loss: 0.2769 - acc: 0.8940 - val_loss: 0.9808 - val_acc: 0.7810\n",
      "Wall time: 15min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten,Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(1000)\n",
    "\n",
    "x_train = np.load('data/dataswedish_leaf64x64pix_train_images.npy')\n",
    "y_train = np.load('data/dataswedish_leaf64x64pix_train_labels.npy')\n",
    "\n",
    "x_test = np.load('data/dataswedish_leaf64x64pix_test_images.npy')\n",
    "y_test = np.load('data/dataswedish_leaf64x64pix_test_labels.npy')\n",
    "\n",
    "\n",
    "\n",
    "# A sequential alexnet\n",
    "alexnet = Sequential()\n",
    "\n",
    "# 1st Convolutional Layer\n",
    "alexnet.add(Conv2D(filters=32, input_shape=(64,64,3), kernel_size=(4,4), padding='same'))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2,2), padding='same'))\n",
    "# Batch Normalisation before passing it to the next layer\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# 2nd Convolutional Layer\n",
    "alexnet.add(Conv2D(filters=96, kernel_size=(4,4), strides=(1,1), padding='same'))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# 3rd Convolutional Layer\n",
    "alexnet.add(Conv2D(filters=128, kernel_size=(4,4), strides=(1,1), padding='same'))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# 4th Convolutional Layer\n",
    "alexnet.add(Conv2D(filters=256, kernel_size=(4,4), strides=(1,1), padding='same'))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# 5th Convolutional Layer\n",
    "alexnet.add(Conv2D(filters=128, kernel_size=(4,4), strides=(1,1), padding='same'))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same'))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# Passing it to a dense layer- full connected layer\n",
    "alexnet.add(Flatten())\n",
    "\n",
    "# 1st Dense Layer\n",
    "alexnet.add(Dense(4096, input_shape=(32*32*3,)))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(Dropout(0.4))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# 2nd Dense Layer\n",
    "alexnet.add(Dense(4096))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(Dropout(0.4))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# 3rd Dense Layer\n",
    "alexnet.add(Dense(1000))\n",
    "alexnet.add(Activation('relu'))\n",
    "alexnet.add(Dropout(0.4))\n",
    "alexnet.add(BatchNormalization())\n",
    "\n",
    "# Output Layer\n",
    "alexnet.add(Dense(16))\n",
    "alexnet.add(Activation('softmax'))\n",
    "\n",
    "alexnet.summary()\n",
    "\n",
    "# Compile \n",
    "alexnet.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "alexnet.fit(x_train, y_train, epochs=10, verbose=1,validation_data=(x_test,y_test), shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "categorical_labels = to_categorical(y, num_classes=None)\n",
    "categorical_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
